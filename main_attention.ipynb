{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7aa1248d-3edc-4c72-98e7-313fc1260e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df4cb328-21bf-4afc-b5fc-92e23a31d370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset from https://www.kaggle.com/datasets/saurabhshahane/electricity-load-forecasting\n",
    "df = pd.read_csv('kaggle_electricity_load_forecasting_train.csv')\n",
    "df = df.set_index('datetime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38d422b6-ff7a-4a29-b068-7bea4f4ce7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_output = np.array(df['nat_demand']).reshape(-1, 1)\n",
    "data_input = np.array(df[['T2M_toc', 'QV2M_toc','TQL_toc','W2M_toc']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e64234d2-0ce5-496e-91cc-d742666b2c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_output = torch.Tensor(data_output).cuda()\n",
    "data_input = torch.Tensor(data_input).cuda()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_input, data_output, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6f7d0e8-0c3f-4cad-8074-1200b4ad9b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionNet(nn.Module):\n",
    "    def __init__(self, inp_size, out_size, linear_size=36):\n",
    "        super(AttentionNet, self).__init__()\n",
    "\n",
    "        self.inp_size = inp_size\n",
    "        self.linear_size = linear_size\n",
    "        self.out_size = out_size\n",
    "        \n",
    "        self.inp_lin = nn.Linear(self.inp_size, self.linear_size)\n",
    "        self.lin1 = nn.Linear(self.linear_size, self.linear_size)\n",
    "        self.lin2 = nn.Linear(self.linear_size, self.linear_size)\n",
    "        self.out_lin = nn.Linear(self.linear_size, self.out_size)\n",
    "    def forward(self, x):\n",
    "        x = self.inp_lin(x)\n",
    "        x1 = F.relu(self.lin1(x))\n",
    "        x2 = F.relu(self.lin2(x))\n",
    "        temp = torch.matmul(x1, torch.matmul(x2.transpose(-2, -1), x2))\n",
    "        x = self.out_lin(temp)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DLFTNet(nn.Module):\n",
    "    def __init__(self, inp_size, out_size, linear_size=36):\n",
    "        super(DLFTNet, self).__init__()\n",
    "\n",
    "        self.inp_size = inp_size\n",
    "        self.linear_size = linear_size\n",
    "        self.out_size = out_size\n",
    "        \n",
    "        self.inp_lin = nn.Linear(self.inp_size, self.linear_size)\n",
    "        self.lin1 = nn.Linear(self.linear_size, self.linear_size)\n",
    "        self.out_lin = nn.Linear(self.linear_size ** 2, self.out_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.inp_lin(x)\n",
    "        x1 = torch.fft.fft(x).real.view(-1, x.shape[1], 1)\n",
    "        x2 = F.relu(self.lin1(x).view(-1, 1, x.shape[1]))\n",
    "        temp = torch.matmul(x1, x2).reshape(-1, x.shape[1] ** 2)\n",
    "        x = self.out_lin(temp)\n",
    "        return x\n",
    "\n",
    "class MLPNet(nn.Module):\n",
    "    def __init__(self, inp_size, out_size, freq_domain=None, linear_size=36):\n",
    "        super(MLPNet, self).__init__()\n",
    "\n",
    "        self.inp_size = inp_size\n",
    "        self.linear_size = linear_size\n",
    "        self.out_size = out_size\n",
    "        self.freq_domain = freq_domain\n",
    "        \n",
    "        self.inp_lin = nn.Linear(self.inp_size, self.linear_size)\n",
    "        self.lin1 = nn.Linear(self.linear_size, self.linear_size)\n",
    "        self.out_lin = nn.Linear(self.linear_size, self.out_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.inp_lin(x)\n",
    "        x1 = F.relu(self.lin1(x))\n",
    "        x = self.out_lin(x1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac9e192b-f58d-43db-b56d-2d6ab98239d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\flyin\\miniconda3\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1182.615478515625\n",
      "100 1177.202880859375\n",
      "200 1169.762451171875\n",
      "300 1160.4730224609375\n",
      "400 1148.1759033203125\n",
      "500 1131.9595947265625\n",
      "600 1111.125244140625\n",
      "700 1085.0704345703125\n",
      "800 1053.261474609375\n",
      "900 1015.2235107421875\n",
      "1000 970.5525512695312\n",
      "1100 918.9031982421875\n",
      "1200 859.9591064453125\n",
      "1300 793.4448852539062\n",
      "1400 719.1085815429688\n",
      "1500 636.7168579101562\n",
      "1600 546.1304321289062\n",
      "1700 448.59893798828125\n",
      "1800 351.4959411621094\n",
      "1900 271.2974853515625\n",
      "2000 221.19187927246094\n",
      "2100 196.92221069335938\n",
      "2200 186.4965362548828\n",
      "2300 181.93414306640625\n",
      "2400 179.2973175048828\n",
      "2500 177.26112365722656\n",
      "2600 175.37591552734375\n",
      "2700 173.523193359375\n",
      "2800 171.66604614257812\n",
      "2900 169.80076599121094\n",
      "3000 167.9314422607422\n",
      "3100 166.061279296875\n",
      "3200 164.20379638671875\n",
      "3300 162.35951232910156\n",
      "3400 160.54598999023438\n",
      "3500 158.76885986328125\n",
      "3600 157.00802612304688\n",
      "3700 155.28018188476562\n",
      "3800 153.59251403808594\n",
      "3900 151.96241760253906\n",
      "4000 150.39871215820312\n",
      "4100 148.89822387695312\n",
      "4200 147.45233154296875\n",
      "4300 146.0731658935547\n",
      "4400 144.75929260253906\n",
      "4500 143.51718139648438\n",
      "4600 142.34359741210938\n",
      "4700 141.244140625\n",
      "4800 140.2125244140625\n",
      "4900 139.25082397460938\n",
      "5000 138.3526153564453\n",
      "5100 137.52130126953125\n",
      "5200 136.75856018066406\n",
      "5300 136.07069396972656\n",
      "5400 135.4491729736328\n",
      "5500 134.8909912109375\n",
      "5600 134.3935089111328\n",
      "5700 133.94918823242188\n",
      "5800 133.55514526367188\n",
      "5900 133.20477294921875\n",
      "6000 132.89114379882812\n",
      "6100 132.60873413085938\n",
      "6200 132.35971069335938\n",
      "6300 132.148681640625\n",
      "6400 131.97335815429688\n",
      "6500 131.82888793945312\n",
      "6600 131.71119689941406\n",
      "6700 131.6144561767578\n",
      "6800 131.532470703125\n",
      "6900 131.46669006347656\n",
      "7000 131.41644287109375\n",
      "7100 131.3776092529297\n",
      "7200 131.3470001220703\n",
      "7300 131.32275390625\n",
      "7400 131.30296325683594\n",
      "7500 131.28550720214844\n",
      "7600 131.26974487304688\n",
      "7700 131.25537109375\n",
      "7800 131.24220275878906\n",
      "7900 131.22946166992188\n",
      "8000 131.21669006347656\n",
      "8100 131.20391845703125\n",
      "8200 131.1912078857422\n",
      "8300 131.17840576171875\n",
      "8400 131.16552734375\n",
      "8500 131.1525115966797\n",
      "8600 131.1393280029297\n",
      "8700 131.1260223388672\n",
      "8800 131.11253356933594\n",
      "8900 131.098876953125\n",
      "9000 131.0850067138672\n",
      "9100 131.07098388671875\n",
      "9200 131.0567626953125\n",
      "9300 131.04238891601562\n",
      "9400 131.02784729003906\n",
      "9500 131.01316833496094\n",
      "9600 130.99832153320312\n",
      "9700 130.9833526611328\n",
      "9800 130.96824645996094\n",
      "9900 130.95298767089844\n",
      "153.15092\n"
     ]
    }
   ],
   "source": [
    "net = MLPNet(4, 1).cuda()\n",
    "opti = torch.optim.Adam(net.parameters(), lr=0.0001)\n",
    "loss = torch.nn.L1Loss().cuda()\n",
    "\n",
    "for i in range(0, 10000) :\n",
    "    opti.zero_grad()\n",
    "    out = net(X_train)\n",
    "    l = loss(out, y_train)\n",
    "    l.backward()\n",
    "    opti.step()\n",
    "    if i % 100 == 0 :\n",
    "        print(i, l.item())\n",
    "\n",
    "out = net(X_test).detach().cpu().numpy()\n",
    "cpu_y_test = y_test.detach().cpu().numpy()\n",
    "print(root_mean_squared_error(cpu_y_test, out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06fd3772-d284-4c1c-80bb-cde90d66791f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 7282236.5\n",
      "100 37487.6015625\n",
      "200 48025.59375\n",
      "300 47266.23828125\n",
      "400 35357.7265625\n",
      "500 39922.109375\n",
      "600 39596.59765625\n",
      "700 25588.37890625\n",
      "800 30273.6640625\n",
      "900 29120.55078125\n",
      "1000 20498.322265625\n",
      "1100 76905.4921875\n",
      "1200 24109.064453125\n",
      "1300 10171.361328125\n",
      "1400 10208.13671875\n",
      "1500 1711.6390380859375\n",
      "1600 3421.7548828125\n",
      "1700 6860.49365234375\n",
      "1800 14253.9873046875\n",
      "1900 4317.369140625\n",
      "2000 3267.496826171875\n",
      "2100 8079.59130859375\n",
      "2200 9878.986328125\n",
      "2300 1971.7557373046875\n",
      "2400 6650.9404296875\n",
      "2500 4765.849609375\n",
      "2600 939.4674682617188\n",
      "2700 3707.892578125\n",
      "2800 5218.16357421875\n",
      "2900 3862.19482421875\n",
      "3000 2993.172119140625\n",
      "3100 5562.0322265625\n",
      "3200 1964.3350830078125\n",
      "3300 3016.974853515625\n",
      "3400 4873.9609375\n",
      "3500 1670.475341796875\n",
      "3600 931.0582275390625\n",
      "3700 3241.307373046875\n",
      "3800 2295.7392578125\n",
      "3900 1980.336181640625\n",
      "4000 1736.53466796875\n",
      "4100 1684.1612548828125\n",
      "4200 1713.9290771484375\n",
      "4300 1749.7264404296875\n",
      "4400 1745.22412109375\n",
      "4500 1716.3428955078125\n",
      "4600 1672.03564453125\n",
      "4700 1592.16552734375\n",
      "4800 1487.6690673828125\n",
      "4900 1404.4365234375\n",
      "5000 1284.4686279296875\n",
      "5100 1230.515380859375\n",
      "5200 1135.3944091796875\n",
      "5300 1038.8743896484375\n",
      "5400 964.4189453125\n",
      "5500 938.8126831054688\n",
      "5600 911.8529052734375\n",
      "5700 901.7857666015625\n",
      "5800 897.4193725585938\n",
      "5900 903.0553588867188\n",
      "6000 919.6337890625\n",
      "6100 931.8171997070312\n",
      "6200 935.9390869140625\n",
      "6300 917.3781127929688\n",
      "6400 922.078125\n",
      "6500 918.9713134765625\n",
      "6600 904.7691040039062\n",
      "6700 896.48779296875\n",
      "6800 903.5827026367188\n",
      "6900 881.6617431640625\n",
      "7000 841.6344604492188\n",
      "7100 806.8463745117188\n",
      "7200 786.9886474609375\n",
      "7300 763.374267578125\n",
      "7400 741.0045166015625\n",
      "7500 718.3859252929688\n",
      "7600 696.8333129882812\n",
      "7700 674.3067016601562\n",
      "7800 652.24365234375\n",
      "7900 629.5006103515625\n",
      "8000 610.4524536132812\n",
      "8100 589.1441040039062\n",
      "8200 569.9715576171875\n",
      "8300 554.7822875976562\n",
      "8400 538.3560180664062\n",
      "8500 527.5569458007812\n",
      "8600 514.0668334960938\n",
      "8700 504.1545715332031\n",
      "8800 499.0046691894531\n",
      "8900 493.57879638671875\n",
      "9000 491.6568603515625\n",
      "9100 500.2133483886719\n",
      "9200 512.1726684570312\n",
      "9300 533.05810546875\n",
      "9400 552.9349365234375\n",
      "9500 575.5418090820312\n",
      "9600 592.3760375976562\n",
      "9700 595.0296020507812\n",
      "9800 583.7518310546875\n",
      "9900 561.2535400390625\n",
      "760.81256\n"
     ]
    }
   ],
   "source": [
    "net = AttentionNet(4, 1).cuda()\n",
    "opti = torch.optim.Adam(net.parameters(), lr=0.0001)\n",
    "loss = torch.nn.L1Loss().cuda()\n",
    "\n",
    "for i in range(0, 10000) :\n",
    "    opti.zero_grad()\n",
    "    out = net(X_train)\n",
    "    l = loss(out, y_train)\n",
    "    l.backward()\n",
    "    opti.step()\n",
    "    if i % 100 == 0 :\n",
    "        print(i, l.item())\n",
    "\n",
    "out = net(X_test).detach().cpu().numpy()\n",
    "cpu_y_test = y_test.detach().cpu().numpy()\n",
    "print(root_mean_squared_error(cpu_y_test, out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4502bd62-e4ee-4316-b083-cbe6d19d5ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1247.81884765625\n",
      "100 196.32151794433594\n",
      "200 115.03426361083984\n",
      "300 114.99043273925781\n",
      "400 114.9762954711914\n",
      "500 114.96565246582031\n",
      "600 114.9582748413086\n",
      "700 114.95230865478516\n",
      "800 114.94759368896484\n",
      "900 114.94313049316406\n",
      "1000 114.93885803222656\n",
      "1100 114.93463134765625\n",
      "1200 114.92959594726562\n",
      "1300 114.92379760742188\n",
      "1400 114.91921997070312\n",
      "1500 114.91461944580078\n",
      "1600 114.91012573242188\n",
      "1700 114.90570831298828\n",
      "1800 114.90118408203125\n",
      "1900 114.8965835571289\n",
      "2000 114.89196014404297\n",
      "2100 114.88724517822266\n",
      "2200 114.8824462890625\n",
      "2300 114.8775405883789\n",
      "2400 114.87251281738281\n",
      "2500 114.86746978759766\n",
      "2600 114.86224365234375\n",
      "2700 114.85684204101562\n",
      "2800 114.8512954711914\n",
      "2900 114.84574127197266\n",
      "3000 114.84011840820312\n",
      "3100 114.83438873291016\n",
      "3200 114.82870483398438\n",
      "3300 114.8231201171875\n",
      "3400 114.8171615600586\n",
      "3500 114.81134033203125\n",
      "3600 114.80547332763672\n",
      "3700 114.79936218261719\n",
      "3800 114.79340362548828\n",
      "3900 114.78734588623047\n",
      "4000 114.78141021728516\n",
      "4100 114.77549743652344\n",
      "4200 114.76959991455078\n",
      "4300 114.76376342773438\n",
      "4400 114.75786590576172\n",
      "4500 114.74974060058594\n",
      "4600 114.7437973022461\n",
      "4700 114.73776245117188\n",
      "4800 114.73164367675781\n",
      "4900 114.7254638671875\n",
      "5000 114.71929931640625\n",
      "5100 114.71297454833984\n",
      "5200 114.70648956298828\n",
      "5300 114.69989776611328\n",
      "5400 114.69312286376953\n",
      "5500 114.6860580444336\n",
      "5600 114.6785888671875\n",
      "5700 114.67071533203125\n",
      "5800 114.66252136230469\n",
      "5900 114.65413665771484\n",
      "6000 114.64517211914062\n",
      "6100 114.6354751586914\n",
      "6200 114.62568664550781\n",
      "6300 114.61463165283203\n",
      "6400 114.60419464111328\n",
      "6500 114.591552734375\n",
      "6600 114.58476257324219\n",
      "6700 114.56605529785156\n",
      "6800 114.55166625976562\n",
      "6900 114.53559112548828\n",
      "7000 114.32164001464844\n",
      "7100 114.21331787109375\n",
      "7200 114.06509399414062\n",
      "7300 113.89144134521484\n",
      "7400 113.71183776855469\n",
      "7500 113.56040954589844\n",
      "7600 113.39457702636719\n",
      "7700 113.26167297363281\n",
      "7800 113.13214111328125\n",
      "7900 113.02623748779297\n",
      "8000 112.93020629882812\n",
      "8100 112.84933471679688\n",
      "8200 112.78321075439453\n",
      "8300 112.73233032226562\n",
      "8400 112.68787384033203\n",
      "8500 112.639404296875\n",
      "8600 112.60286712646484\n",
      "8700 112.57035064697266\n",
      "8800 112.6543197631836\n",
      "8900 112.51409912109375\n",
      "9000 112.49115753173828\n",
      "9100 112.46770477294922\n",
      "9200 112.43769836425781\n",
      "9300 112.39776611328125\n",
      "9400 112.30461120605469\n",
      "9500 111.65875244140625\n",
      "9600 111.44197082519531\n",
      "9700 111.24200439453125\n",
      "9800 111.07390594482422\n",
      "9900 110.92463684082031\n",
      "137.8292\n"
     ]
    }
   ],
   "source": [
    "net = DLFTNet(4, 1).cuda()\n",
    "opti = torch.optim.Adam(net.parameters(), lr=0.0001)\n",
    "loss = torch.nn.L1Loss().cuda()\n",
    "\n",
    "for i in range(0, 10000) :\n",
    "    opti.zero_grad()\n",
    "    out = net(X_train)\n",
    "    l = loss(out, y_train)\n",
    "    l.backward()\n",
    "    opti.step()\n",
    "    if i % 100 == 0 :\n",
    "        print(i, l.item())\n",
    "\n",
    "out = net(X_test).detach().cpu().numpy()\n",
    "cpu_y_test = y_test.detach().cpu().numpy()\n",
    "print(root_mean_squared_error(cpu_y_test, out))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
